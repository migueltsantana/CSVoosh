{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Datoosh What is it? Datoosh is a Python tool to help you upload CSV files into SQL databases. It makes use of the multiprocessing library to open multiple connections and insert all data more efficiently. Documentation: https://migueltsantana.github.io/datoosh/ Source code: https://github.com/migueltsantana/Datoosh Project layout databases/ mysql.py # The MySQL wrapper module. postgresql.py # The PostgreSQL wrapper module. sqlite.py # The SQLite wrapper module. main.py # The main script. requirements.txt # The pip dependencies of the application.","title":"Datoosh"},{"location":"#datoosh","text":"","title":"Datoosh"},{"location":"#what-is-it","text":"Datoosh is a Python tool to help you upload CSV files into SQL databases. It makes use of the multiprocessing library to open multiple connections and insert all data more efficiently. Documentation: https://migueltsantana.github.io/datoosh/ Source code: https://github.com/migueltsantana/Datoosh","title":"What is it?"},{"location":"#project-layout","text":"databases/ mysql.py # The MySQL wrapper module. postgresql.py # The PostgreSQL wrapper module. sqlite.py # The SQLite wrapper module. main.py # The main script. requirements.txt # The pip dependencies of the application.","title":"Project layout"},{"location":"dbms/","text":"Supported DBMS MySQL PostgreSQL SQLite Where is my DBMS? This tool is very focused on popular DBMS, at the current time. Nevertheless, you are free to contribute with your favourite DBMS! Just make a pull request with your DBMS and I'll be more than happy to include it in this repository. To start creating the DBMS wrapper for your favourite DBMS, take a look at the existing ones and create one with the same methods. If after your implementation, you are able to upload data to the database, you're good to go! Tip Make sure you maintain the method's signatures as they all need to be the same, across all DBMS. You can make auxiliary methods, as long as you keep them inside one .py file","title":"Supported DBMS"},{"location":"dbms/#supported-dbms","text":"MySQL PostgreSQL SQLite","title":"Supported DBMS"},{"location":"dbms/#where-is-my-dbms","text":"This tool is very focused on popular DBMS, at the current time. Nevertheless, you are free to contribute with your favourite DBMS! Just make a pull request with your DBMS and I'll be more than happy to include it in this repository. To start creating the DBMS wrapper for your favourite DBMS, take a look at the existing ones and create one with the same methods. If after your implementation, you are able to upload data to the database, you're good to go! Tip Make sure you maintain the method's signatures as they all need to be the same, across all DBMS. You can make auxiliary methods, as long as you keep them inside one .py file","title":"Where is my DBMS?"},{"location":"installation/","text":"Installation Requirements Python 3.6+ Commands Create the virtual environment python3 -m venv venv Activate the virtual environment source venv/bin/activate Install all the dependencies pip install -r requirements.txt Successfully installed all dependencies Done! You're ready to use it!","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Python 3.6+","title":"Requirements"},{"location":"installation/#commands","text":"Create the virtual environment python3 -m venv venv Activate the virtual environment source venv/bin/activate Install all the dependencies pip install -r requirements.txt Successfully installed all dependencies Done! You're ready to use it!","title":"Commands"},{"location":"options/","text":"CLI Options & Usage This tool is solely a command-line (CLI) application. Follow the options below to use it correctly. Options -f / --file -w / --max-worker-threads -s / --settings -d / --delimiter -f Required : Yes Alternative flag : --file Description : The CSV file to process -w Required : Yes Alternative flag : --max-worker-threads Description : The maximum number of concurrent processes to read and process the CSV file -s Required : Yes Alternative flag : --settings Description : The settings file -d Required : No Alternative flag : --delimiter Description : The delimiter of the CSV file Default value : , Usage Make sure the virtual environment is activated! To run this CLI, you will need some dependencies that have been previously installed. If you haven't started your installation, please consider checking Installation . To activate your virtual environment, you just have to make sure you're on the root folder of the project and type source venv/bin/activate . python main.py -w 50 -s settings.yaml -f file.csv Successfully uploaded to the database","title":"CLI Options & Usage"},{"location":"options/#cli-options-usage","text":"This tool is solely a command-line (CLI) application. Follow the options below to use it correctly.","title":"CLI Options &amp; Usage"},{"location":"options/#options","text":"-f / --file -w / --max-worker-threads -s / --settings -d / --delimiter","title":"Options"},{"location":"options/#-f","text":"Required : Yes Alternative flag : --file Description : The CSV file to process","title":"-f"},{"location":"options/#-w","text":"Required : Yes Alternative flag : --max-worker-threads Description : The maximum number of concurrent processes to read and process the CSV file","title":"-w"},{"location":"options/#-s","text":"Required : Yes Alternative flag : --settings Description : The settings file","title":"-s"},{"location":"options/#-d","text":"Required : No Alternative flag : --delimiter Description : The delimiter of the CSV file Default value : ,","title":"-d"},{"location":"options/#usage","text":"Make sure the virtual environment is activated! To run this CLI, you will need some dependencies that have been previously installed. If you haven't started your installation, please consider checking Installation . To activate your virtual environment, you just have to make sure you're on the root folder of the project and type source venv/bin/activate . python main.py -w 50 -s settings.yaml -f file.csv Successfully uploaded to the database","title":"Usage"},{"location":"settings/","text":"Settings file File definition The settings file is provided to the tool with the flag -s (or --settings ). This will be a YAML file and you need to use the following structure with this tool: table-name : The name of the table that you want to put the data into. columns : The description of the columns. For each column: name : The name of the column. type : The type of data. Be careful with the data definition! These data definition types will be used to instantiate a new table . They must be aligned with the DBMS you'll be using. If you want to define a primary key or a unique constraint, this will the correct place to do it. database : The description of the database. type : The type of the DBMS you'll be using ( mysql , postgresql or sqlite ). host : The hostname of the DBMS. user : The username to access the DBMS. password : The password to access the DBMS. name : The name of the database to use. Example file table-name: data columns: - name: event_place type: VARCHAR(100) - name: event_type type: VARCHAR(100) ... database: type: mysql host: localhost user: root password: password name: datoosh","title":"Settings file"},{"location":"settings/#settings-file","text":"","title":"Settings file"},{"location":"settings/#file-definition","text":"The settings file is provided to the tool with the flag -s (or --settings ). This will be a YAML file and you need to use the following structure with this tool: table-name : The name of the table that you want to put the data into. columns : The description of the columns. For each column: name : The name of the column. type : The type of data. Be careful with the data definition! These data definition types will be used to instantiate a new table . They must be aligned with the DBMS you'll be using. If you want to define a primary key or a unique constraint, this will the correct place to do it. database : The description of the database. type : The type of the DBMS you'll be using ( mysql , postgresql or sqlite ). host : The hostname of the DBMS. user : The username to access the DBMS. password : The password to access the DBMS. name : The name of the database to use.","title":"File definition"},{"location":"settings/#example-file","text":"table-name: data columns: - name: event_place type: VARCHAR(100) - name: event_type type: VARCHAR(100) ... database: type: mysql host: localhost user: root password: password name: datoosh","title":"Example file"},{"location":"why/","text":"Why? Data is growing faster and faster and exponentially. It has been crucial to process these large amounts of data at record speeds. However, data is often found in many other formats or locations other than a database. Because of this, it's important to ensure that they are housed in these technologies and enable their immediate analysis. Is there a need for it? When there is a need to put large amounts of data stored in CSV files, this tool is very useful to get the databases populated in a short time. This tool uses Python's multiprocessing library to slit the file into multiple parts, make connections to the database and parallelize tasks.","title":"Why?"},{"location":"why/#why","text":"Data is growing faster and faster and exponentially. It has been crucial to process these large amounts of data at record speeds. However, data is often found in many other formats or locations other than a database. Because of this, it's important to ensure that they are housed in these technologies and enable their immediate analysis.","title":"Why?"},{"location":"why/#is-there-a-need-for-it","text":"When there is a need to put large amounts of data stored in CSV files, this tool is very useful to get the databases populated in a short time. This tool uses Python's multiprocessing library to slit the file into multiple parts, make connections to the database and parallelize tasks.","title":"Is there a need for it?"}]}